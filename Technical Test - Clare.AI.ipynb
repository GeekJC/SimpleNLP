{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Test - Clare.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a technical test with a few tasks to complete, the coding can be done either python 2 or 3.\n",
    "\n",
    "For each task, it can be implemented and documented in Jupyter Notebook or a seperate .py file\n",
    "\n",
    "For all the functions defined, it shall be within a class called class SentenceSimilarity():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part involves how to crawl data from webpages\n",
    "\n",
    "Suggests tools to use: \n",
    "Beautiful soup, Scarpy (https://scrapy.org)\n",
    "\n",
    "Crawl the questions and answers from the following page\n",
    "\n",
    "https://www.cncbinternational.com/personal/investments/securities-trading-service-guide-and-faq/tc/index.jsp\n",
    "\n",
    "The output format should be in the CSV with following columns\n",
    "\n",
    "Category, Question, Answer, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class CNCBQuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://www.cncbinternational.com/personal/investments/securities-trading-service-guide-and-faq/tc/index.jsp',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.xpath('span/small/text()').extract_first(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(\"href\")').extract_first()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "            \n",
    "    parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is to build language specific model for simliarity comparison later. Word2Vec is a powerful deep learning models that google used to compare text similarity, however it requires big data and computing power to build one\n",
    "\n",
    "For Chinese, it requires chinese tokenizer to break sentences into words\n",
    "\n",
    "Jieba“结巴”中文分词 is the popular tool in python\n",
    "https://github.com/fxsjy/jieba\n",
    "\n",
    "To build language model, gensim is popular and highly scalable\n",
    "https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenize questions into words\n",
    "\n",
    "Define a function to tokenize the questions from 1 into words using Jieba, it might require custom dictionary to make it correctly. Jieba has built-in dictionary but it's optimized for simplified chinese, so for words in cantonese, it would need to add it manually in the custom dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build a TFIDF model using questions and answers\n",
    "\n",
    "Build a TFIDF model using questions and answers from part 1, together with the function in 2.1\n",
    "\n",
    "Reference to build the model\n",
    "\n",
    "https://radimrehurek.com/gensim/tutorial.html\n",
    "\n",
    "https://radimrehurek.com/gensim/tut2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Similarity Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for question simliarity comparison\n",
    "\n",
    "def similarity(self,sentence):\n",
    "\n",
    "the input is sentence, where it will be tokenized first and then compare against the model defined in 2.2\n",
    "\n",
    "With using TFIDF, each document will be represented as bag-of-words counts and applies a weighting. Reference - Last paragraph https://radimrehurek.com/gensim/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER (命名实体), short for Named Entity Recognition is probably the first step towards information extraction from unstructured text.\n",
    "\n",
    "It basically means extracting what is a real world entity from the text (Person, Organization, Event etc …).\n",
    "\n",
    "There are few popular libraries which support in Chinese: Stanford NLP/HanL\n",
    "https://nlp.stanford.edu/software/CRF-NER.shtml\n",
    "\n",
    "https://github.com/hankcs/HanLP\n",
    "\n",
    "https://github.com/hankcs/HanLP/wiki/%E8%AE%AD%E7%BB%83%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B\n",
    "\n",
    "Define a function to extract and print Named Entity on the input sentence\n",
    "\n",
    "def get_entities(self, sentence)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}