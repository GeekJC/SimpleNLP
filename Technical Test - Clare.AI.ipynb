{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Test - Clare.AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a technical test with a few tasks to complete, the coding can be done either python 2 or 3.\n",
    "\n",
    "For each task, it can be implemented and documented in Jupyter Notebook or a seperate .py file\n",
    "\n",
    "For all the functions defined, it shall be within a class called class SentenceSimilarity():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part involves how to crawl data from webpages\n",
    "\n",
    "Suggests tools to use: \n",
    "Beautiful soup, Scarpy (https://scrapy.org)\n",
    "\n",
    "Crawl the questions and answers from the following page\n",
    "\n",
    "https://www.cncbinternational.com/personal/investments/securities-trading-service-guide-and-faq/tc/index.jsp\n",
    "\n",
    "The output format should be in the CSV with following columns\n",
    "\n",
    "Category, Question, Answer, Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Language Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is to build language specific model for simliarity comparison later. Word2Vec is a powerful deep learning models that google used to compare text similarity, however it requires big data and computing power to build one\n",
    "\n",
    "For Chinese, it requires chinese tokenizer to break sentences into words\n",
    "\n",
    "Jieba“结巴”中文分词 is the popular tool in python\n",
    "https://github.com/fxsjy/jieba\n",
    "\n",
    "To build language model, gensim is popular and highly scalable\n",
    "https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenize questions into words\n",
    "\n",
    "Define a function to tokenize the questions from 1 into words using Jieba, it might require custom dictionary to make it correctly. Jieba has built-in dictionary but it's optimized for simplified chinese, so for words in cantonese, it would need to add it manually in the custom dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.529 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "jieba.initialize()\n",
    "jieba.set_dictionary('dict.txt.big.txt')\n",
    "\n",
    "\n",
    "def tokenize_question(file):\n",
    "    questions = []\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "    for text in data.Question:\n",
    "        words = jieba.cut(text)\n",
    "        tokens = ','.join(words)\n",
    "        questions.append(tokens.split(','))\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build a TFIDF model using questions and answers\n",
    "\n",
    "Build a TFIDF model using questions and answers from part 1, together with the function in 2.1\n",
    "\n",
    "Reference to build the model\n",
    "\n",
    "https://radimrehurek.com/gensim/tutorial.html\n",
    "\n",
    "https://radimrehurek.com/gensim/tut2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,481 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,485 : INFO : built Dictionary(676 unique tokens: ['網上', '理財', ' ', '/', '流動']...) from 192 documents (total 3274 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,485 : INFO : saving Dictionary object under CNCB.dict, separately None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,486 : INFO : saved CNCB.dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,490 : INFO : storing corpus in Matrix Market format to CNCB.mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,491 : INFO : saving sparse matrix to CNCB.mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,491 : INFO : PROGRESS: saving document #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,495 : INFO : saved 192x676 matrix, density=1.904% (2471/129792)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,496 : INFO : saving MmCorpus index to CNCB.mm.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,497 : INFO : collecting document frequencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,497 : INFO : PROGRESS: processing document #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,498 : INFO : calculating IDF weights for 192 documents and 675 features (2471 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,499 : INFO : using serial LSI version on this node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,500 : INFO : updating model with new documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,503 : INFO : preparing a new chunk of documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,505 : INFO : using 100 extra samples and 2 power iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,506 : INFO : 1st phase: constructing (676, 500) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,511 : INFO : orthonormalizing (676, 500) action matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,546 : INFO : 2nd phase: running dense svd on (500, 192) matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,553 : INFO : computing the final decomposition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,553 : INFO : keeping 189 factors (discarding 0.000% of energy spectrum)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,555 : INFO : processed documents up to #192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,556 : INFO : topic #0(3.933): -0.843*\" \" + -0.162*\"I\" + -0.155*\"?\" + -0.122*\"the\" + -0.121*\"\" + -0.111*\"What\" + -0.103*\"can\" + -0.099*\"order\" + -0.091*\"Securities\" + -0.089*\"is\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,557 : INFO : topic #1(2.640): -0.323*\"我\" + -0.273*\"可以\" + -0.189*\"证券\" + -0.186*\"的\" + -0.184*\"證券\" + -0.170*\"渠道\" + -0.169*\"或\" + -0.164*\"买卖\" + -0.164*\"，\" + -0.160*\"買賣\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,557 : INFO : topic #2(1.958): 0.350*\"证券\" + 0.299*\"买卖\" + 0.295*\"沪\" + 0.290*\"什么\" + 0.266*\"股通\" + 0.220*\"于\" + -0.161*\"如果\" + -0.161*\"想\" + -0.161*\"做\" + -0.152*\"或\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,558 : INFO : topic #3(1.929): -0.362*\"證券\" + -0.335*\"買賣\" + -0.284*\"什麼\" + -0.281*\"於\" + -0.243*\"滬股通\" + -0.204*\"渠道\" + 0.172*\"或\" + -0.163*\"美國\" + 0.161*\"交易\" + 0.144*\"想\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,559 : INFO : topic #4(1.892): -0.338*\"交易\" + 0.202*\"可以\" + -0.183*\"安排\" + 0.174*\"做\" + 0.174*\"想\" + 0.171*\"如果\" + -0.169*\"交收\" + -0.154*\"有\" + 0.147*\"我\" + 0.146*\"正在\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,559 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-26 14:08:12,569 : INFO : creating matrix with 192 documents and 189 features\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) \n",
    "\n",
    "questions = tokenize_question('CNCBCrawlling/quotes_products.csv')\n",
    "dictionary = corpora.Dictionary(questions)\n",
    "dictionary.save('CNCB.dict')\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in questions]\n",
    "corpora.MmCorpus.serialize('CNCB.mm', corpus)\n",
    "\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=400)\n",
    "\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Similarity Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for question simliarity comparison\n",
    "\n",
    "def similarity(self,sentence):\n",
    "\n",
    "the input is sentence, where it will be tokenized first and then compare against the model defined in 2.2\n",
    "\n",
    "With using TFIDF, each document will be represented as bag-of-words counts and applies a weighting. Reference - Last paragraph https://radimrehurek.com/gensim/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(42, 1)]\n[(3, 0.68763924), (101, 0.056910545), (15, 0.030518956), (5, 0.029729415), (19, 0.028071254), (122, 0.025732804), (133, 0.025271848), (103, 0.025137503), (52, 0.023241285), (4, 0.022675358), (24, 0.020066548), (170, 0.018275268), (50, 0.017309155), (178, 0.01718832), (100, 0.017077576), (177, 0.017041652), (20, 0.016628839), (22, 0.016509712), (12, 0.015742214), (49, 0.015342262), (136, 0.015261725), (134, 0.015122531), (25, 0.013574764), (32, 0.013573896), (46, 0.013445649), (176, 0.013203029), (118, 0.013190262), (48, 0.012876686), (62, 0.012839887), (11, 0.012825537), (18, 0.012536442), (102, 0.012059662), (28, 0.011912441), (188, 0.011683389), (119, 0.01156247), (2, 0.011163667), (39, 0.01047378), (132, 0.010230407), (155, 0.010187369), (43, 0.0099757705), (167, 0.0094788708), (180, 0.0094556268), (171, 0.0091288202), (151, 0.0089919381), (67, 0.0086237807), (10, 0.0082975775), (187, 0.0082548903), (9, 0.0062697716), (117, 0.0058897585), (0, 0.0055863932), (173, 0.0054644402), (174, 0.0054415632), (63, 0.0053400062), (30, 0.0052960529), (153, 0.0051427651), (45, 0.0050188387), (14, 0.0047072247), (33, 0.0043724491), (116, 0.0039268937), (1, 0.0035462454), (183, 0.0033286475), (189, 0.0031406395), (36, 0.0027889181), (40, 0.0027719289), (161, 0.002536023), (66, 0.0024776496), (99, 0.0022599529), (131, 0.0022144364), (8, 0.0020690784), (31, 0.0019882326), (181, 0.0017361529), (37, 0.0015844656), (168, 0.0015520845), (184, 0.0014819242), (152, 0.0012896843), (61, 0.001118904), (35, 0.0010808222), (29, 0.0010482147), (165, 0.00090574659), (6, 0.0008370392), (65, 0.00080743246), (51, 0.00079919398), (190, 0.00070103304), (38, 0.00058769062), (166, 0.00050677173), (53, 0.00028969906), (47, 0.00011243671), (23, -4.4174492e-05), (115, -9.123981e-05), (13, -0.00031565502), (44, -0.00032841414), (17, -0.00036854483), (138, -0.00057999045), (21, -0.00079307705), (182, -0.00087343622), (26, -0.00093467161), (7, -0.001165133), (169, -0.0013426486), (68, -0.001349844), (172, -0.0014171917), (42, -0.0014735069), (154, -0.0014901813), (186, -0.0016878992), (175, -0.0016971026), (191, -0.0018140618), (137, -0.0019030869), (104, -0.0019438528), (16, -0.0021413341), (121, -0.0021609738), (34, -0.002200786), (130, -0.0022740029), (164, -0.0024170484), (41, -0.0024438445), (120, -0.0026487), (135, -0.0029321462), (163, -0.0031314893), (149, -0.0033496725), (142, -0.0036638726), (114, -0.0037620901), (162, -0.0043476964), (179, -0.0045277718), (64, -0.0046278723), (69, -0.0046666814), (125, -0.0048699919), (83, -0.0049801208), (94, -0.0052507236), (79, -0.0052872943), (143, -0.0053248024), (85, -0.0054832008), (141, -0.0055458285), (185, -0.0055465791), (82, -0.0056114318), (106, -0.0056986981), (84, -0.0056991158), (77, -0.0057391822), (148, -0.0057948772), (96, -0.0058147023), (139, -0.0058553857), (93, -0.0058894665), (95, -0.0059144478), (145, -0.0059351036), (59, -0.0059555732), (60, -0.0059669553), (127, -0.0060023069), (128, -0.006083318), (111, -0.0060982411), (75, -0.0061091054), (158, -0.0061318004), (105, -0.0061413702), (129, -0.0061579999), (90, -0.006208044), (71, -0.0062381011), (146, -0.0062824897), (123, -0.0062869634), (126, -0.0062978333), (80, -0.006301662), (87, -0.0063037965), (159, -0.0063072494), (57, -0.0063293045), (160, -0.0063457927), (27, -0.0063665723), (110, -0.0063970964), (55, -0.006443996), (76, -0.0064516058), (107, -0.0064550554), (73, -0.0064647594), (74, -0.0064647594), (156, -0.0065506632), (78, -0.006610123), (157, -0.0066424981), (72, -0.0066659139), (112, -0.0066663586), (147, -0.0066991337), (124, -0.0066991895), (86, -0.0067225127), (70, -0.0067386469), (58, -0.0067401025), (144, -0.0067520356), (140, -0.0067561949), (98, -0.0067630354), (97, -0.0067925993), (56, -0.0068125385), (109, -0.0068320315), (81, -0.0068388907), (113, -0.0071000946), (150, -0.0071477816), (92, -0.0072102807), (91, -0.0072613657), (108, -0.0074958559), (89, -0.007889377), (88, -0.0079588741), (54, -0.0098078996)]\nDictionary(676 unique tokens: ['網上', '理財', ' ', '/', '流動']...)\n"
     ]
    }
   ],
   "source": [
    "def similarity(sentence):\n",
    "    vec = dictionary.doc2bow(sentence.lower().split())\n",
    "    print(vec)\n",
    "    vec_lsi = lsi[vec]\n",
    "    sims = index[vec_lsi]\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    print(sims)\n",
    "    \n",
    "similarity(sentence='購買力')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER (命名实体), short for Named Entity Recognition is probably the first step towards information extraction from unstructured text.\n",
    "\n",
    "It basically means extracting what is a real world entity from the text (Person, Organization, Event etc …).\n",
    "\n",
    "There are few popular libraries which support in Chinese: Stanford NLP/HanL\n",
    "https://nlp.stanford.edu/software/CRF-NER.shtml\n",
    "\n",
    "https://github.com/hankcs/HanLP\n",
    "\n",
    "https://github.com/hankcs/HanLP/wiki/%E8%AE%AD%E7%BB%83%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B\n",
    "\n",
    "Define a function to extract and print Named Entity on the input sentence\n",
    "\n",
    "def get_entities(self, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}